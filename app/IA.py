import json
from difflib import get_close_matches
from sentence_transformers import SentenceTransformer, util
from app import app
import google.generativeai as genai
import os
import glob
import time
import random

pasta_docs = "Docs/Artigos"

try:
    genai.configure(api_key=app.config['API'])
    modelo_gemini = genai.GenerativeModel("gemini-2.0-flash-lite")
    gemini_disponivel = True
    print("Gemini configurado com sucesso!")
except Exception as e:
    print(f"Erro na configuraÃ§Ã£o do Gemini: {e}")
    gemini_disponivel = False

modelo_emb = SentenceTransformer('all-MiniLM-L6-v2')

arquivos_json = glob.glob(os.path.join(pasta_docs, "artigo_*.json"))
dados = []
for arquivo in arquivos_json:
    try:
        with open(arquivo, "r", encoding="utf-8") as f:
            artigos = json.load(f)
            dados.extend(artigos)
            print(f"Arquivo {arquivo} carregado com sucesso!")
    except Exception as e:
        print(f"Erro ao carregar {arquivo}: {e}")

if not dados:
    print("Nenhum dado foi carregado. Verifique os arquivos JSON.")
    exit()

topicos = [item["topico"] for item in dados]
respostas = [item["resposta"] for item in dados]
emb_topicos = modelo_emb.encode(topicos, convert_to_tensor=True)

def busca_semantica(entrada):
    try:
        emb_entrada = modelo_emb.encode(entrada, convert_to_tensor=True)
        similaridades = util.cos_sim(emb_entrada, emb_topicos)
        index = similaridades.argmax().item()
        similaridade = similaridades[0][index].item()
        
        print(f"Similaridade encontrada: {similaridade:.3f}")
        
        if similaridade >= 0.78: 
            return respostas[index], similaridade
        return None, similaridade
    except Exception as e:
        print(f"Erro na busca semÃ¢ntica: {e}")
        return None, 0

def resposta_fallback_local(pergunta, dados_contexto):
    palavras_chave = pergunta.lower().split()
    respostas_relevantes = []
    
    for item in dados_contexto:
        topico_lower = item["topico"].lower()
        resposta = item["resposta"]
        
        correspondencias = sum(1 for palavra in palavras_chave if palavra in topico_lower)
        if correspondencias > 0:
            respostas_relevantes.append((resposta, correspondencias))
    
    if respostas_relevantes:
        respostas_relevantes.sort(key=lambda x: x[1], reverse=True)
        return respostas_relevantes[0][0]
    
    return "Desculpe, nÃ£o encontrei informaÃ§Ãµes especÃ­ficas sobre isso na base de conhecimento."

def UsarGemini(texto):
    if gemini_disponivel:
        try:
            contexto = "\n".join([f"TÃ³pico: {t['topico']}\nResposta: {t['resposta']}" for t in dados])
            
            prompt = f"""
            VocÃª Ã© um assistente especialista em programaÃ§Ã£o e tecnologia. 

            **Contexto da base de conhecimento:**
            {contexto}

            **InstruÃ§Ãµes:**
            2. Use o contexto acima como referÃªncia, mas nÃ£o se limite apenas a ele
            3. Se for algo muito especÃ­fico que nÃ£o estÃ¡ no contexto, responda com seu conhecimento geral
            4. Seja educado, claro e direto
            5. Responda em no maximo 47 palavras

            **Pergunta do usuÃ¡rio:** {texto}

            **Sua resposta:**
            """

            # Adicionar delay para evitar rate limiting
            time.sleep(2 + random.uniform(0, 1))
            
            resposta_gerada = modelo_gemini.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=100,
                    top_p=0.9
                )
            )
            if resposta_gerada.candidates:
                texto = resposta_gerada.candidates[0].content.parts[0].text
                texto = texto.replace("**", "")
            else:
                texto = "Nenhuma resposta gerada."
            return f"ğŸ¤– Resposta do bot:\n{texto}"
            
        except Exception as e:
            print(f"Erro no Gemini: {e}")
            # Fallback para resposta local
            return f"âš ï¸ Gemini indisponÃ­vel. Resposta local:\n{resposta_fallback_local(texto, dados)}"
    else:
        # Fallback completo para resposta local
        return f"ğŸ“‹ Resposta da base local:\n{resposta_fallback_local(texto, dados)}"

def CriarChamadoParaBanco(text):
    """
    Gera automaticamente um ticket estruturado (title, description, prioridade)
    a partir de um texto livre do usuÃ¡rio.
    """
    try:
        # HeurÃ­stica simples para prioridade
        prioridade = "mÃ©dia"
        texto_lower = text.lower()
        if any(p in texto_lower for p in ["urgente", "imediato", "crÃ­tico", "falha total", "parado"]):
            prioridade = "alta"
        elif any(p in texto_lower for p in ["quando possÃ­vel", "sem pressa", "melhoria", "sugestÃ£o"]):
            prioridade = "baixa"
        
        if gemini_disponivel:
            # Usar Gemini para enriquecer o ticket
            prompt = f"""
            VocÃª Ã© um assistente de suporte tÃ©cnico.  
            Dado o texto do usuÃ¡rio: "{text}"

            Crie um ticket estruturado em JSON com os seguintes campos:
            - title: tÃ­tulo breve e descritivo
            - description: descriÃ§Ã£o mais detalhada
            - prioridade: alta, mÃ©dia ou baixa (com base no texto)

            Retorne somente o JSON vÃ¡lido.
            """
            resposta = modelo_gemini.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=150,
                    top_p=0.9
                )
            )
            try:
                ticket = json.loads(resposta.text)
                return ticket
            except:
                # Fallback caso Gemini nÃ£o retorne JSON vÃ¡lido
                return {
                    "title": text[:50] + ("..." if len(text) > 50 else ""),
                    "description": text,
                    "prioridade": prioridade
                }
        else:
            # Fallback completo: sÃ³ heurÃ­stica local
            return {
                "title": text[:50] + ("..." if len(text) > 50 else ""),
                "description": text,
                "prioridade": prioridade
            }

    except Exception as e:
        print(f"Erro ao criar chamado: {e}")
        return {
            "title": "Erro ao processar chamado",
            "description": f"Texto original: {text}",
            "prioridade": "mÃ©dia"
        }

def responder_usuario(texto):
    print(f"\nğŸ” Processando: '{texto}'")
    
    # Sempre tenta busca semÃ¢ntica primeiro (gratuita)
    resposta_semantica, similaridade = busca_semantica(texto)
    if resposta_semantica and similaridade > 0.7:
        return f"{resposta_semantica}"
    
    # Se nÃ£o encontrou boa correspondÃªncia semÃ¢ntica, pergunta se quer usar Gemini
    return False