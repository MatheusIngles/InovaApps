import json
from difflib import get_close_matches
from sentence_transformers import SentenceTransformer, util
import google.generativeai as genai
import os
import glob
import time
import random

pasta_docs = "Docs/Artigos"

# ConfiguraÃ§Ã£o do Gemini (com fallback)
try:
    genai.configure(api_key="AIzaSyAlM1mTfkfsGuoaban70ZjEi2NB1_tx4mM")
    # Tentar usar modelos mais leves para economizar quota
    modelo_gemini = genai.GenerativeModel("gemini-2.0-flash-lite")
    gemini_disponivel = True
    print("Gemini configurado com sucesso!")
except Exception as e:
    print(f"Erro na configuraÃ§Ã£o do Gemini: {e}")
    gemini_disponivel = False

# Modelo de embeddings local (sempre disponÃ­vel)
modelo_emb = SentenceTransformer('all-MiniLM-L6-v2')

# Carregar todos os arquivos JSON
arquivos_json = glob.glob(os.path.join(pasta_docs, "artigo_*.json"))
dados = []
for arquivo in arquivos_json:
    try:
        with open(arquivo, "r", encoding="utf-8") as f:
            artigos = json.load(f)
            dados.extend(artigos)
            print(f"Arquivo {arquivo} carregado com sucesso!")
    except Exception as e:
        print(f"Erro ao carregar {arquivo}: {e}")

# Verificar se hÃ¡ dados carregados
if not dados:
    print("Nenhum dado foi carregado. Verifique os arquivos JSON.")
    exit()

# Preparar embeddings
topicos = [item["topico"] for item in dados]
respostas = [item["resposta"] for item in dados]
emb_topicos = modelo_emb.encode(topicos, convert_to_tensor=True)

# FunÃ§Ã£o de busca semÃ¢ntica melhorada
def busca_semantica(entrada):
    try:
        emb_entrada = modelo_emb.encode(entrada, convert_to_tensor=True)
        similaridades = util.cos_sim(emb_entrada, emb_topicos)
        index = similaridades.argmax().item()
        similaridade = similaridades[0][index].item()
        
        print(f"Similaridade encontrada: {similaridade:.3f}")
        
        if similaridade > 0.84:  # Threshold reduzido
            return respostas[index], similaridade
        return None, similaridade
    except Exception as e:
        print(f"Erro na busca semÃ¢ntica: {e}")
        return None, 0

# FunÃ§Ã£o fallback local para quando o Gemini nÃ£o estÃ¡ disponÃ­vel
def resposta_fallback_local(pergunta, dados_contexto):
    """Resposta simples baseada em palavras-chave quando o Gemini nÃ£o estÃ¡ disponÃ­vel"""
    
    palavras_chave = pergunta.lower().split()
    respostas_relevantes = []
    
    for item in dados_contexto:
        topico_lower = item["topico"].lower()
        resposta = item["resposta"]
        
        # Verificar correspondÃªncia de palavras-chave
        correspondencias = sum(1 for palavra in palavras_chave if palavra in topico_lower)
        if correspondencias > 0:
            respostas_relevantes.append((resposta, correspondencias))
    
    if respostas_relevantes:
        # Ordenar por relevÃ¢ncia e retornar a melhor
        respostas_relevantes.sort(key=lambda x: x[1], reverse=True)
        return respostas_relevantes[0][0]
    
    return "Desculpe, nÃ£o encontrei informaÃ§Ãµes especÃ­ficas sobre isso na base de conhecimento."

def UsarGemini(texto):
    if gemini_disponivel:
        try:
            contexto = "\n".join([f"TÃ³pico: {t['topico']}\nResposta: {t['resposta']}" for t in dados])
            
            prompt = f"""
            VocÃª Ã© um assistente especialista em programaÃ§Ã£o e tecnologia. 

            **Contexto da base de conhecimento:**
            {contexto}

            **InstruÃ§Ãµes:**
            2. Use o contexto acima como referÃªncia, mas nÃ£o se limite apenas a ele
            3. Se for algo muito especÃ­fico que nÃ£o estÃ¡ no contexto, responda com seu conhecimento geral
            4. Seja educado, claro e direto

            **Pergunta do usuÃ¡rio:** {texto}

            **Sua resposta:**
            """

            # Adicionar delay para evitar rate limiting
            time.sleep(2 + random.uniform(0, 1))
            
            resposta_gerada = modelo_gemini.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.6,
                    max_output_tokens=100,
                    top_p=0.9
                )
            )
            
            return f"ğŸ¤– Resposta do Gemini:\n{resposta_gerada.text}"
            
        except Exception as e:
            print(f"Erro no Gemini: {e}")
            # Fallback para resposta local
            return f"âš ï¸ Gemini indisponÃ­vel. Resposta local:\n{resposta_fallback_local(texto, dados)}"
    else:
        # Fallback completo para resposta local
        return f"ğŸ“‹ Resposta da base local:\n{resposta_fallback_local(texto, dados)}"

def CriarChamadoParaBanco(text):
    print("Criando chamado para o banco de dados...")

# FunÃ§Ã£o principal para responder usuÃ¡rio - CORRIGIDA
def responder_usuario(texto):
    print(f"\nğŸ” Processando: '{texto}'")
    
    # Sempre tenta busca semÃ¢ntica primeiro (gratuita)
    resposta_semantica, similaridade = busca_semantica(texto)
    if resposta_semantica and similaridade > 0.7:
        return f"{resposta_semantica}"
    
    # Se nÃ£o encontrou boa correspondÃªncia semÃ¢ntica, pergunta se quer usar Gemini
    return False